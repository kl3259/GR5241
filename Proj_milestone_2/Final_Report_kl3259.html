<!DOCTYPE html>
<html>
<head>
<title>Final_Report_kl3259.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h6 id="gr5241-spring-2022">GR5241 Spring 2022</h6>
<h1 id="project-milestone-2-deep-learning-part"><strong>Project Milestone 2: Deep Learning Part</strong></h1>
<h2 id="name-kangshuo-li-uni-kl3259"><strong>Name: Kangshuo Li                 UNI: kl3259</strong></h2>
<br>
<h2 id="part-4-deep-learning"><strong>Part 4: Deep Learning</strong></h2>
<br>
<h3 id="3-5-points-train-a-single-layer-neural-network-with-100-hidden-units-eg-with-architecture-784-%E2%86%92-100-%E2%86%92-10-you-should-use-the-initialization-scheme-discussed-in-class-and-choose-a-reasonable-learning-rate-ie-01-train-the-network-repeatedly-more-than-5-times-using-different-random-seeds-so-that-each-time-you-start-with-a-slightly-different-initialization-of-the-weights-run-the-optimization-for-at-least-150-epochs-each-time-if-you-observe-underfitting-continue-training-the-network-for-more-epochs-until-you-start-seeing-overfitting"><strong>3. (5 points) Train a single layer neural network with 100 hidden units (e.g. with architecture: 784 → 100 → 10). You should use the initialization scheme discussed in class and choose a reasonable learning rate (i.e. 0.1). Train the network repeatedly (more than 5 times) using different random seeds, so that each time, you start with a slightly different initialization of the weights. Run the optimization for at least 150 epochs each time. If you observe underfitting, continue training the network for more epochs until you start seeing overfitting.</strong></h3>
<br>
<pre><code>The structure of single layer neural network is:
single_layer_NN(
(flatten): Flatten(start_dim=1, end_dim=-1)
(single_layer_nn): Sequential(
(0): Linear(in_features=784, out_features=100, bias=True)
(1): Sigmoid()
(2): Linear(in_features=100, out_features=10, bias=True)
)
)
</code></pre>
<p>In this case we only construct one hidden layer with 784 original inputs, 100 neurons and the sigmoid function as activation function. The output size is 10 which corresponds to the number of classes in MNIST dataset.</p>
<br>
<h4 id="a-plot-the-average-training-cross-entropy-error-sum-of-the-cross-entropy-error-terms-over-the-training-dataset-divided-by-the-total-number-of-training-example-on-the-y-axis-vs-the-epoch-number-x-axis-on-the-same-figure-plot-the-average-validation-cross-entropy-error-function-examine-the-plots-of-training-error-and-validationtest-error-generalization-how-does-the-networks-performance-differ-on-the-training-set-versus-the-validation-set-during-learning-use-the-plot-of-training-and-testing-error-curves-to-support-your-argument"><strong>(a)</strong> Plot the average training cross-entropy error (sum of the cross-entropy error terms over the training dataset divided by the total number of training example) on the y-axis vs. the epoch number (x-axis). On the same figure, plot the average validation cross-entropy error function. Examine the plots of training error and validation/test error (generalization). How does the network’s performance differ on the training set versus the validation set during learning? Use the plot of training and testing error curves to support your argument.</h4>
<br>
<img src = "./learning_curve_Cross_entropy_error.png" height = "500" />
<p>The leanring curves with respect to cross-entropy error of all 6 random initializations are shown above.</p>
<p>In this question, we set the learning rate $l = 0.1$, and $N_{epochs} = 200$ with batch size $N_{batch} = 64$. Since we used a setup that requires sufficient epochs to attain a status like overfitting, we can note that most of the training processes have a pattern of overfitting. The training errors are smaller than the test error. In axes of seed 42, 422, 442, 4422, and 4442, the cross-entropy loss started from different values with different initializations and decreased quickly in the first 25 epochs, then as the number of training epochs increased, the cross-entropy error slowly decreased to 0 with some fluctuations while the validation(test set here) cross-entropy error slightly increased from 0.2 to around 0.3, so these plots show that Their corresponding training process involves overfitting. In other words, the single neural networks began to become weaker for generalization while still working well on the training set.</p>
<p>Also note that the training process with seed 4222 got an unique learning curve, which includes a much higher initial cross-entropy loss and the learning process has several cliff drops and ends up with no significant overfitting pattern. This could be the result of a relatively unlucky initialization with parameters that were far from the local optimal point. Finally it also got the training error close to 0 and validation error close to 0.3, but the time cost to attained same performance is higher than the other random initializations.</p>
<br>
<h4 id="b-we-could-implement-an-alternative-performance-measure-to-the-cross-entropy-the-mean-miss-classification-error-we-can-consider-the-output-correct-if-the-correct-label-is-given-a-higher-probability-than-the-incorrect-label-then-count-up-the-total-number-of-examples-that-are-classified-incorrectly-divided-by-the-total-number-of-examples-according-to-this-criterion-for-training-and-validation-respectively-and-maintain-this-statistic-at-the-end-of-each-epoch-plot-the-classification-error-in-percentage-vs-number-of-epochs-for-both-training-and-testing-do-you-observe-a-different-behavior-compared-to-the-behavior-of-the-cross-entropy-error-function"><strong>(b)</strong> We could implement an alternative performance measure to the cross entropy, the mean miss-classification error. We can consider the output correct if the correct label is given a higher probability than the incorrect label, then count up the total number of examples that are classified incorrectly (divided by the total number of examples) according to this criterion for training and validation respectively, and maintain this statistic at the end of each epoch. Plot the classification error (in percentage) vs. number of epochs, for both training and testing. Do you observe a different behavior compared to the behavior of the cross-entropy error function?</h4>
<br>
<img src = "./learning_curve_Misclassification_error.png" height = "500"/>
<p>The learning curves with respect to misclassification error of all 6 random initializations are shown above.</p>
<p>The learning curves of average misclassification error are pretty similar to learning curves based on cross-entropy error. They have the same quick decreasing sections in the first 25 epochs and overfitting sections corresponding to the remaining epochs, and all the fluctuations are at the same pace. Seed 4222 also got cliff drops in this learning curve and takes a longer time to earn the same performance as other neural networks.</p>
<p>However, note that the overfitting parts of these plots are different from those in cross-entropy loss, the validation means misclassification error with seeds 42, 422, 442, 4422, and 4442 nearly maintaining the same value with some little noise in the overfitting phase, and the training loss of mean misclassification keep decreasing to 0. In the previous question, the cross-entropy error on test set was slowly increasing. If we measure the ability of generalization of the neural networks by using mean misclassification error, then this ability is not damaged by overfitting.</p>
<br>
<h4 id="c-visualize-your-best-results-of-the-learned-w-as-one-hundred-28%C3%9728-images-plot-all-filters-as-one-image-as-we-have-seen-in-class-do-the-learned-features-exhibit-any-structure"><strong>(c)</strong> Visualize your best results of the learned W as one hundred 28×28 images (plot all filters as one image, as we have seen in class). Do the learned features exhibit any structure?</h4>
<br>
<img src = "./3(c)_table.png" height = "150" />
<p>The table of evaluation metrics is shown above.</p>
<p>We select the best model based on mean misclassification error on test set standing for the test accuracy and the cross-entropy error on test set. Singel layer neural network with seed 42 has the lowest mean test misclassification error and relatively low test cross-entropy error, so we identify it as the best model.</p>
<img src = "./param_best_model.png" height = "600"/>
<p>This is the visualization of the parameters learned from the best model with seed 42. It's clear that the most frequent pattern is the feature like a shape of &quot;3&quot; with a shade like a shape of &quot;8&quot;. There are also some chaotic features and some features with only part of the number or strokes shown in the restored features. The patterns like &quot;3&quot; frequently appeared might be the result from severely overfitting, as the learning process led the weights to a local optimal that mainly recognizing &quot;3&quot; or &quot;8&quot;.</p>
<br>
<h4 id="d-try-different-values-of-the-learning-rate-you-should-start-with-a-learning-rate-of-01-you-should-then-reduce-it-to-01-and-increase-it-to-02-and-05-what-happens-to-the-convergence-properties-of-the-algorithm-looking-at-both-average-cross-entropy-and--incorrect-try-momentum-of-00-05-09-how-does-momentum-affect-convergence-rate-how-would-you-choose-the-best-value-of-these-parameters"><strong>(d)</strong> Try different values of the learning rate. You should start with a learning rate of 0.1. You should then reduce it to .01, and increase it to 0.2 and 0.5. What happens to the convergence properties of the algorithm (looking at both average cross entropy and % incorrect)? Try momentum of 0.0, 0.5, 0.9. How does momentum affect convergence rate? How would you choose the best value of these parameters?</h4>
<br>
<img src = "./learning_curve_lr_mmt_Cross_entropy_error.png" height = "1200"/>
<img src = "./learning_curve_lr_mmt_Misclassification_error.png" height = "1200"/>
<p>We trained the best model with random seed 42 on the grid of learning rate $lr = {0.01, 0.02, 0.05, 0.1, 0.2, 0.5}$ and momentum $momemtum = {0.0, 0.5, 0.9}$.</p>
<p>According to these 2 figures, both cross-entropy error and mean misclassification error have the same learning pattern when the learning rate and momentum are the same. With fixed momentum, a larger learning rate would lead to higher error rate and higher gap between training and testing error in terms of both the cross-entropy error and misclassification error. With a fixed learning rate, as the momentum increases, we can see that the learning curves are becoming less stable and even cannot make optimization on the loss function or even diverge. One important thing here is that single layer neural networks with lower learning rates seem to have better convergence rates than those with large learning rates.</p>
<img src = "./3(d)_table.png" height = "400"/>
<p>This table shows the evaluation metrics of all single layer NNs trained on the learning rate and momentum grid with the best random seed 42 picked in 3(c). The best parameters should be chosen by the learning curve that have stable learning process and higher test accuracy and lower test cross-entropy error. Since the value of average cross-entropy error on test set is sensitive to overfitting, we can choose the best single layer neural network with parameters:</p>
<ul>
<li>$Seed = 42, lr = 0.01, momentum = 0.0$</li>
</ul>
<p>since it got the lowest test average cross-entropy error and the highest test accuracy.</p>
<br>
<h3 id="4-5-points-redo-part-3a---3d-with-a-cnn-ie-with-one-2-d-convolutional-layers-%E2%86%92-relu-activation-%E2%86%92-maxpooling-with-appropriate-hyperparameters-compare-the-best-result-from-the-single-layer-neural-network-and-the-cnn-what-could-you-conclude"><strong>4. (5 points) Redo part 3(a) - 3(d) with a CNN i.e. with one 2-D convolutional layers → Relu activation → Maxpooling with appropriate hyperparameters. Compare the best result from the single layer neural network and the CNN, what could you conclude?</strong></h3>
<br>
<pre><code>The structure of the single convolutional layer CNN is:
CNN(
(conv): Sequential(
    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
)
(flatten): Flatten(start_dim=1, end_dim=-1)
(single_layer_nn): Sequential(
    (0): Linear(in_features=3136, out_features=10, bias=True)
)
)
</code></pre>
<p>We define the only convolutional layer by using $5 \times 5$ kernel size and a padding of 2 in both vertical and horizontal direction and moves the convolution kernel with stride 1. This conv2d layer would output the data in shape $(16, 28, 28)$. Then after ReLU activation, the features would be reduced to $(16, 14, 14)$ by the MaxPooling layer and flattened by the Flatten layer in to a fully connected layer. The number of output neurons is set to be the number of classes.</p>
<h4 id="a--b"><strong>(a) &amp; (b)</strong></h4>
<br>
<img src = "./learning_curve_cnn_Cross_entropy_error_CNN.png" height = "500"/>
<img src = "./learning_curve_cnn_Misclassification_error_CNN.png" height = "500"/>
<p>The learning curves of CNNs are listed above. The training parameters are: $lr = 0.001$, $N_{batch} = 100$, $N_{epoch} = 80$, $momentum = 0$. I reduced the number of epochs since the CNN is more suitable for computer vision problems, thus it would perform better than single layer NN, we can observe the overfitting under fewer epochs.</p>
<p>We can still see that the patterns in single layer NNs' learning curve also appear in CNNs' figure. The single convolutional layer CNN has the convergence rate as same as the single layer NN, both types can attain a status of overfitting at around the 20th epoch. Note that both two types of training errors and testing errors are lower than single layer NN when the training processes are finished, which conforms with the conclusion that CNN has better performance on image classification than single layer NN. The main reason is that its convolutional layer can work as a feature extractor and maxpooling layer can be interpreted as dimension reduction.</p>
<p>Also, there's less fluctuation during the training process in CNNs' learning curve.</p>
<br>
<h4 id="c"><strong>(c)</strong></h4>
<br>
<img src = "./4(c)_table.png" height = "150"/>
<p>Model comparison between CNNs is based on this table.</p>
<p>All CNNs have similar overfitting patterns starting from around the 10th epoch. The best model is CNN with seed 42 as it has the lowest test average cross-entropy error and lowest mean misclassification error.</p>
<img src = "./param_best_model_cnn.png" height = "600"/>
<p>The visualization of the 16 convolutional kernel from the best single convolutional layer CNN shows some of the structures like the part of strokes with black and white pixels. The kernels are seem to be uncorrelated, which are reasonable.</p>
<br>
<h4 id="d"><strong>(d)</strong></h4>
<img  src = "./learning_curve_lr_mmt_cnn_Cross_entropy_error_CNN.png" height = "1200"/>
<img src = "./learning_curve_lr_mmt_cnn_Misclassification_error_CNN.png" height = "1200"/>
<p>The learning curves with respect to two types of errors on the learning rate and momentum grid are shown above.</p>
<p>We trained the best single convolutional layer CNN with random seed 42 on the grid of learning rate $lr = {0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01}$ and momentum $momemtum = {0.0, 0.5, 0.9}$. Since we already observed the overfitting around the 20th epoch in 4(a) &amp; 4(b), here the number of epoch is reduced to 60 to speed up the training.</p>
<p>Comparing to the training result of single layer NN on the grid, we can see that CNNs are more sensitive to the momentum value, with sufficient epochs, the gap between training error and testing error is enlarged significantly as the momentum increases. The plots in the bottom of the two pictures demonstrate that CNN need to use smaller learning rate and momentum to converge. Suitable learning rate for single layer NN would result in remarkbale fluctuation when applying to CNNs.</p>
<img src = "./4(d)_table.png" height = "400"/>
<p>The final result after training is listed above. We can choose the best single convolutional layer CNN with:</p>
<ul>
<li>$Seed = 42, lr = 0.0001, momentum = 0.5$</li>
</ul>
<p>since this model nearly has no overfitting in its learning curve, and has the lowest test average cross-entropy error and the 3rd highest test accuracy.</p>
<br>
<h3 id="5-5-points-redo-part-3a---3d-with-your-favorite-deep-learning-architecture-eg-introducing-batch-normalization-introducing-dropout-in-training-to-beat-the-performance-of-svm-with-gaussian-kernel-ie-to-have-a-test-error-rate-lower-than-14"><strong>5. (5 points) Redo part 3(a) - 3(d) with your favorite deep learning architecture (e.g., introducing batch normalization, introducing dropout in training) to beat the performance of SVM with Gaussian Kernel, i.e., to have a test error rate lower than 1.4%.</strong></h3>
<br>
<pre><code>lenet(
(conv): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
)
(flatten): Flatten(start_dim=1, end_dim=-1)
(fully_conncet): Sequential(
    (0): Linear(in_features=400, out_features=120, bias=True)
    (1): ReLU()
    (2): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Linear(in_features=120, out_features=84, bias=True)
    (4): ReLU()
    (5): Linear(in_features=84, out_features=10, bias=True)
)
)
</code></pre>
<p>We use Lenet5-like CNN for this question, hereinafter called it lenet. I introduced batch-normalization in both of the convolutional and fully connected part. Evolved from the single convolutional layer CNN, the classic LeNet5 includes more convolutional, maxpooling and fully connected layers, which aims to improve the performance of feature extraction. More parameters makes this lenet more complex and thus would be suitable for image classification. The $28 \times 28$ image was shaped in a sequence: $(6,28,28)$ -&gt; $(6,14,14)$ -&gt; $(16,10,10)$ -&gt; $(16,5,5)$ -&gt; $16 \times 5 \times 5$ -&gt; $120$ -&gt; $84$ -&gt; $10$.</p>
<h4 id="a--b"><strong>(a) &amp; (b)</strong></h4>
<br>
<img src = "./learning_curve_lenet_Cross_entropy_error_LeNet.png" height = "500"/>
<img src = "./learning_curve_lenet_Misclassification_error_LeNet.png" height = "500"/>
<p>The learning curves of the LeNet5-like CNN are shown above. $N_{batch} = 100$, $lr = 0.0005$, $seeds = {42, 422, 442, 4422, 4442}$. I chose the $N_{epochs} = 30$ because the lenet is foreseeable more powerful than the single convolutional layer CNN. More parameters would make it overfitting easily and early. As we can see, lenets start overfitting at around the 5th epoch and easily attain a level of test accuracy of around 99%. The convergence can be attained instantly within 4 epochs. One difference is that the test average misclassification error seems to be more unstable than the average cross-entropy error. Other remaining patterns of overfitting are the same as we found in questions 3 and 4.</p>
<br>
<h4 id="c"><strong>(c)</strong></h4>
<br>
<img src = "./5(c)_table.png" height = "150"/>
<p>The table of evaluation metrics of lenets is shown above.
Since all lenets are overfitting, so we pick the best model to be lenet with seed 4422, since it has nearly the highest test accuracy of 99.13% while with the lowest test cross-entropy error 0.028544, which is more robust against overfitting.</p>
<img src = "./param_best_model_lenet_conv_kernel_0.png" height = "400"/>
<img src = "./param_best_model_lenet_samples_conv_kernel_1.png" height = "400" />
<p>In this question I plotted the first and second convolutional kernels. The figure named kernel_0 shows all the 6 kernels learnt by lenet. For the second convolutional layer, I sampled 6 kernels to represent their structures. We can see that these kernels mainly contain the local structures still like part of strokes while remain uncorrelated.</p>
<br>
<h4 id="d"><strong>(d)</strong></h4>
<br>
<img src = "./learning_curve_lr_mmt_lenet_Cross_entropy_error_LeNet.png" height = "1200"/>
<img src = "./learning_curve_lr_mmt_lenet_Misclassification_error_LeNet.png" height = "1200"/>
<p>The learning curves of the best lenet with respect to two types of errors on the learning rate and momentum grid are shown above.</p>
<p>The parameters grid is :$lr = {0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01}$, $momentum = {0.0, 0.5, 0.9}$, $seed = 4422$. Lenets have the similar property with CNNs(i.e. Learning rate should be small, otherwise there would be great fluctuations and even diverge. With fixed learning rate, larger momentum would be more unstable. ) However, due to the model complexity a lenet has, we find that it can always reach the 99% test accuracy and nearly 0 training loss when it converges in 30 epochs, no matter what momentum and learning rate we use, this represents the advantage that lenet have a better structure over single layer CNNs.</p>
<img src = "./5(d)_table.png" height = "500"/>
<p>The final grid search result is shown in this table. <strong>Most of the test results with a test accuracy over 99% already beat the SVM with Gaussian kernel.</strong> We simply choose:</p>
<ul>
<li>$lr = 0.0001, momentum = 0.9, seed = 4422$</li>
</ul>
<p>as our best lenet with lowest test cross-entropy error 0.032757	and highest test accuracy 99.22%.</p>
<br>
<h2 id="part-5-more-about-deep-learning"><strong>Part 5: More About Deep Learning</strong></h2>
<br>
<h3 id="6-1-points-as-a-warm-up-question-load-the-data-and-plot-a-few-examples-decide-if-the-pixels-were-scanned-out-in-row-major-or-column-major-order-what-is-the-relationship-between-the-2-digits-and-the-last-coordinate-of-each-line"><strong>6. (1 points) As a warm up question, load the data and plot a few examples. Decide if the pixels were scanned out in row-major or column-major order. What is the relationship between the 2 digits and the last coordinate of each line?</strong></h3>
<br>
<img src = "./two_digits.png" height = "500"/>
<p>I split the last column as the labels for this new data. The plot is made by extracting each row of the first 1568 entries and using NumPy.reshape() function to reshape them into $28 \times 56$ array. Since the NumPy function is in a row-oriented order, so the new data is stored in row order. By observing the labels, I found that the labels are the sum of the two digits shown in one picture.</p>
<br>
<h3 id="7-8-points-repeat-part-3a---3d-with-at-least-two-of-your-favorite-deep-learning-architecture-eg-introducing-batch-normalization-introducing-dropout-in-training-with-respect-to-with-traintxt-valtxt-and-testtxt-in-particular"><strong>7. (8 points) Repeat part 3(a) - 3(d) with at least two of your favorite deep learning architecture (e.g., introducing batch normalization, introducing dropout in training) with respect to with train.txt, val.txt and test.txt. In particular,</strong></h3>
<blockquote>
<p>(a) Using train.txt to train your models.</p>
<p>(b) Using the validation error (i.e., the performance on val.txt) to select the best model.</p>
<p>(c) Report the generalization error (i.e., the performance on test.txt) for the model you picked. How would you compare the test errors you obtained with respect to the original MNIST data? Explain why you cannot obtain a test error lower than 1%.</p>
</blockquote>
<br>
<p><strong>LeNet</strong></p>
<pre><code>lenet_alt(
(conv): Sequential(
    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)
)
(dropout): Dropout(p=0.25, inplace=False)
(flatten): Flatten(start_dim=1, end_dim=-1)
(fully_conncet): Sequential(
    (0): Linear(in_features=960, out_features=512, bias=True)
    (1): ReLU()
    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Linear(in_features=512, out_features=128, bias=True)
    (4): ReLU()
    (5): Linear(in_features=128, out_features=19, bias=True)
)
)
</code></pre>
<br>
<p><strong>AlexNet</strong></p>
<pre><code>alexnet(
(conv): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (10): ReLU()
    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
)
(flatten): Flatten(start_dim=1, end_dim=-1)
(fully_connect): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=4608, out_features=1024, bias=True)
    (2): ReLU()
    (3): Linear(in_features=1024, out_features=512, bias=True)
    (4): ReLU()
    (5): Linear(in_features=512, out_features=19, bias=True)
)
)
</code></pre>
<p>We use Lenet5-like CNN and AlexNet-like CNN for this question, hereinafter called them lenet and alexnet respectively. I also introduced dropout and batch-normalization in this question. The lenet is updated by adding a dropout layer before the flatten layer with a dropout probability 0.25 for slight regularization. Since this question has more features to extract, I implemented a modified AlexNet to try to achieve better results than lenet since it has more complex structure, which is designed for more complex hypotheses. To fit the alexnet for the MNIST dataset, I changed the kernel size of maxpooling layer from 3 to 2, and reduced the number of kernels in convolutional to $64, 128, 256$ rather than $96, 256, 384$. The fully connected layer is also simplified, their size is down to $1024, 512$. The output layer size is assigned to 19, which is the number of classes in this new dataset.</p>
<h4 id="a--b"><strong>(a) &amp; (b)</strong></h4>
<br>
<p><strong>LeNet</strong></p>
<img src = "./learning_curve_lenet_alt_Cross_entropy_error_LeNet_alt.png" height = "500"/>
<img src = "./learning_curve_lenet_alt_Misclassification_error_LeNet_alt.png" height = "500"/>
<p>The learning curve of updated lenet is shown above.</p>
<p>The parameters are: $lr = 0.0005$, $seeds = {42, 422, 442, 4222, 4422, 4442}$, $momentum = 0$, $N_{batch} = 100$, $N_{epochs} = 30$. We can see that the updated lenet is stable and the overfitting is limited, there'are no increment of validation cross-entropy error. All different random seeds yield similar learning curve. The validation loss functions of the udpated lenet is a little higher than working on original MNIST.</p>
<p><strong>AlexNet</strong></p>
<img src = "./learning_curve_alexnet_Cross_entropy_error_AlexNet.png" height = "500"/>
<img src = "./learning_curve_alexnet_Misclassification_error_AlexNet.png" height = "500"/>
<p>The learning curve of alexnet is shown above.</p>
<p>The parameters are: $lr = 0.001$, $seeds = {42, 422, 442, 4222, 4422, 4442}$, $momentum = 0$, $N_{batch} = 100$, $N_{epochs} = 30$. Alexnets have unique learning curves, as their loss function on both training set and the validation set remain high at the first 5 - 10 epochs, and then rapidly decrease to a level under 0.5 average cross-entropy error and 10% average misclassification error on both training and validation set. In the whole process, alexnets are extremely stable and faced no overfitting. This demonstrates the powerful learning ability and well-defined structure of alexnet.</p>
<h4 id="c"><strong>(c)</strong></h4>
<br>
<p><strong>LeNet</strong></p>
<img src = "./7(c)_lenet_alt_table.png" height = "150"/>
<p>Best udpated lenet is the one with seed 42, it has the highest validation accuracy 93.16% and the lowest validation average cross-entropy error 0.240117.</p>
<img src = "./param_best_model_lenet_alt_conv_kernel_0.png" height = "400"/>
<img src = "./param_best_model_lenet_alt_sample_conv_kernel_1.png" height = "400"/>
<p>The visualization of two convolutional kernel in the best updated lenent are shown above. The second layer I choose only 6 samples to be displayed. They are similar with previous one in CNNs and lenets with some heterogeneous structures but also remain uncorrelated.</p>
<p><strong>AlexNet</strong></p>
<img src = "./7(c)_alexnet_table.png" height = "150"/>
<p>The best alexnet is from seed 4442 with the highest validation accuracy 96.84% and the lowest validation average cross-entropy error 0.142802. The difference between updated lenet and alexnet also confirms that alexnet is more powerful.</p>
<img src = "./param_best_model_alexnet_samples_conv_kernel_0.png" height = "400"/>
<img src = "./param_best_model_alexnet_samples_conv_kernel_1.png" height = "400"/>
<p>The samples of convolutional kernels of the best parameters learned in alexnet are shown above.</p>
<p>The 12 $3 \times 3$ kernels also show different structures without any correlation, thus they are plausible.</p>
<h4 id="d"><strong>(d)</strong></h4>
<br>
<p><strong>LeNet</strong></p>
<img src = "./learning_curve_lr_mmt_lenet_alt_Cross_entropy_error_LeNet_alt.png" height = "1200"/>
<img src = "./learning_curve_lr_mmt_lenet_alt_Misclassification_error_LeNet_alt.png" height = "1200"/>
<p>Above are the learning curves of updated lenet based on the best random seed 42 on the grid of learning rate and momentum.</p>
<p>The grid is $lr = {0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01}$, $momentum = {0.0, 0.5, 0.9}$. We can see that the updated lenets are sensitive to learning rate. When learning rate is small(i.e. 0.0001) and without any acceleration of momentum, its learning curve would have a longer learning phase with gradient descent optimizations lasting about 30 epochs, which is clearly shown on the top left subplot. As it's a kind of CNN, it also has the property we discussed in the previous question. The convergence rate in this question is more strict on learning rate, if the learning rate is too large(i.e. 0.01), it even has a great fluctuation that makes it diverge after a temporary convergence.</p>
<p>While the training and validation metrics are pretty close in cases that would converge, a good parameter combination for lenet to learn is using small learning rate with high momentum to achieve stable and fast converge rate.</p>
<img src = "./7(d)_lenet_alt_table.png" height = "400"/>
<p>According to this grid search result, the best updated lenet should use:</p>
<ul>
<li>$seed = 42, lr = 0.0005, momentum = 0.5$</li>
</ul>
<p>as its learning parameters to get the highest validation accuracy 92.62% and a low validation cross-entropy error.
<br></p>
<p><strong>AlexNet</strong></p>
<img src = "./learning_curve_lr_mmt_alexnet_Cross_entropy_error_AlexNet.png" height = "1200"/>
<img src = "./learning_curve_lr_mmt_alexnet_Misclassification_error_AlexNet.png" height = "1200"/>
<p>The learning curve of grid search is on $lr  = {0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01}$ and $momentum = {0.0, 0.5, 0.9}$. According to the subplots, alexnets are more sensitive to the learning rate even than the updated lenet in the previous question. Small learning rates like 0.0001 cannot let it converge in the first 30 epochs, while large learning rates like 0.005 and 0.01 cannot allow it to make optimization properly on this training dataset. The available range of learning rates for alexnet is the most narrow one we tried so far. Relatively large learning rates with large momentum would result in heavy fluctuations and divergence.</p>
<p>Some subplots show nothing or strange patterns, this might be the result of gradient explosion.</p>
<img src = "./7(d)_alexnet_table.png" height = "400"/>
<p>The grid search result of alexnet</p>
<ul>
<li>$seed = 4442, lr = 0.002, momentum = 0.0$</li>
</ul>
<p>finally attained the highest validation accuracy about 97.02% and beat the updated lenet.</p>
<br>
<h4 id="generalization-error"><strong>Generalization Error</strong></h4>
<br>
<p><strong>LeNet</strong></p>
<pre><code>Generalization result of lenet_alt:
Test Accuracy:  93.60%|Test Loss - cross entropy:    0.236467|Test Loss - mis-clf:    0.064000 
</code></pre>
<p><strong>AlexNet</strong></p>
<pre><code>Generalization result of AlexNet:
Test Accuracy:  97.24%|Test Loss - cross entropy:    0.152874|Test Loss - mis-clf:    0.027600
</code></pre>
<p>We finally use our best updated lenet and alexnet model picked in 7(d) to make predictions on the test set and give the generalization error. Note that updated lenet got validation cross-entropy error 0.294697 and validation accuracy 92.62%, alexnet got 0.176864 and 97.02% respectively. Both of the two best models outperform on the test data, and thus there are no overfitting problems in the training processes. However, both of the neural network structures got weaker performance than those working on original MNIST dataset. The best result on original MNIST dataset is the lowest test cross-entropy error of 0.032757 and the highest test accuracy of 99.22%</p>
<br>
<p><strong>Explain why you cannot obtain a test error lower than 1%.</strong>
The number of classes raised from 10 to 19, and the number of raw features are doubled in this question, which makes the real-world hypothesis more complex, thus it is natural that the classification performance got worse. If we assume that the features from a specific number are independent, as the class conditional assumption in naive Bayes, then the neural networks working on two digits would have the best test accuracy at $0.9922^2 \approx 0.9845$ to predict both digits correctly as two independent neural networks working on their own digit, which is less than 99%.</p>

</body>
</html>
